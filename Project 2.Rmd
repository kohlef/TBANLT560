---
title: "Project 2"
author: "Kohle Ferenchak"
date: "3/14/2021"
output: word_document
---
#Uploading the data
```{r}
#load the mlbench package which has the BreastCancer data set
require(mlbench)

# if you don't have any required package, use the install.packages() command
# load the data set
data(BreastCancer)
set.seed(225)
```

#Here taking a first look at the data, we see if there is any cleaning that needs to be done. There are some missing values in our dataset that need to be dealt with. By using na.omit it removes these rows from the dataset. This will keep our data balanced and allow for the models to run.
```{r}
#summary(BreastCancer)
data(BreastCancer)
mydata <- cbind(BreastCancer[11],BreastCancer[1:10])
BreastCancer <- na.omit(BreastCancer)
BreastCancer$Id <- NULL
```

# In the first model we look at the support vector machine (SVM) to investivating if a tumor is benign or malignant. It misclassifies 13 benign tumors as malignant, and more seriously it misclassifies 8 malignant tumors as benign.
```{r}
library(e1071)
mysvm <- svm(Class ~ ., BreastCancer)
mysvm.pred <- predict(mysvm, BreastCancer)
table(mysvm.pred,BreastCancer$Class)
```

# In the second model we look at the Naive Bayes classifier. It misclassifies 13 benign tumors as malignant which is the same as the SVM model. But Naive Bayes misclassifies only 3 malignant tumors as benign which is signifcant better than the SVM model.
```{r}
library(klaR)
mynb <- NaiveBayes(Class ~ ., BreastCancer)
mynb.pred <- predict(mynb,BreastCancer)
table(mynb.pred$class,BreastCancer$Class)
```

# In the third model we look at a neural network. It misclassifies 2 benign tumors as malignant which is the best model yet for this type of misclassification. The Neural network misclassifies only 4 malignant tumors as benign which is the same at the Niave Bayes classifier. This is the best single model thus far.
```{r}
library(nnet)
mynnet <- nnet(Class ~ ., BreastCancer, size=1)
mynnet.pred <- predict(mynnet,BreastCancer,type="class")
table(mynnet.pred,BreastCancer$Class)
library(MASS)
```

# In the fourth model we look at a decsion tree. It misclassifies 13 benign tumors as malignant, and 9 malignant tumors as benign. This is the poorest accuracy result yet.
```{r}
#Decision trees
library(rpart)
mytree <- rpart(Class ~ ., BreastCancer)
plot(mytree); text(mytree) 
summary(mytree)
mytree.pred <- predict(mytree,BreastCancer,type="class")
table(mytree.pred,BreastCancer$Class)
```

# In the fifth model we look at Leave-1-Out Cross Validation (LOOCV). It misclassifies 14 benign tumors as malignant, and 20 malignant tumors as benign. This is now by far the least accuract single model that we have seen.
```{r}
# Leave-1-Out Cross Validation (LOOCV)
ans <- numeric(length(BreastCancer[,1]))
for (i in 1:length(BreastCancer[,1])) {
  mytree <- rpart(Class ~ ., BreastCancer[-i,])
  mytree.pred <- predict(mytree,BreastCancer[i,],type="class")
  ans[i] <- mytree.pred
}
ans <- factor(ans,labels=levels(BreastCancer$Class))
table(ans,BreastCancer$Class)
# The same as above in this case
```

# In the sixth model we look at Regularised Discriminant Analysis. It misclassifies 11 benign tumors as malignant and only 2 malignant tumors as benign. This is our best model to not misclassify malignant tumors as benign. 
```{r}
#Regularised Discriminant Analysis
library(klaR)
myrda <- rda(Class ~ ., BreastCancer)
myrda.pred <- predict(myrda, BreastCancer)
table(myrda.pred$class,BreastCancer$Class)
```

# In the seventh model we look at Random Forests. This model suspicously got all of the tumors properly categorized. This is by far the best model that we have seen so far.
```{r}
#Random Forests
library(randomForest)
myrf <- randomForest(Class ~ .,BreastCancer)
myrf.pred <- predict(myrf, BreastCancer)
table(myrf.pred, BreastCancer$Class)
```

#Here I make sure that all of my prediction are the same length so that they can run in ensemble.
```{r}
#Update all predicitons to full dataset
myrf.predFull <- predict(myrf, BreastCancer)
myrda.predFull <- predict(myrda, BreastCancer)
mytree.predFull <- predict(mytree,BreastCancer,type="class")
mynnet.predFull <- predict(mynnet,BreastCancer,type="class")
mysvm.predFull <- predict(mysvm, BreastCancer)
mynb.predFull <- predict(mynb,BreastCancer)
```

# Now combine all of the models into one using the Majority Rule ensemble. This ensemble model that balacnces the strengths of all the models which should create the best model at accurately predicting. In actuality the SVM was slighly more accurate and the neural network was the best reasonable model. The decsion forest did end up correctly classifying all of the tumors.
```{r}
combine.classes<-data.frame(myrf.predFull, myrda.predFull$class,
mytree.predFull,mynnet.predFull,mysvm.predFull, mynb.predFull$class)
#head(combine.classes)
#head(myrf.pred)
#head(myrda.pred)
combine.classes$myrf.predFull<-ifelse(combine.classes$myrf.pred=="benign", 0, 1)
combine.classes[,2]<-ifelse(combine.classes[,2]=="benign", 0, 1)
combine.classes[,3]<-ifelse(combine.classes[,3]=="benign", 0, 1)
combine.classes[,4]<-ifelse(combine.classes[,4]=="benign", 0, 1)
combine.classes[,5]<-ifelse(combine.classes[,5]=="benign", 0, 1)
combine.classes[,6]<-ifelse(combine.classes[,6]=="benign", 0, 1)
majority.vote=rowSums(combine.classes)
#head(majority.vote)
combine.classes[,7]<-rowSums(combine.classes)
combine.classes[,8]<-ifelse(combine.classes[,7]>=4, "malignant", "benign")
table(combine.classes[,8], BreastCancer$Class)
```

